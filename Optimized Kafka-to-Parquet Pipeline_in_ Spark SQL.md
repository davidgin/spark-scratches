üöÄ Optimized Kafka-to-Iceberg Pipeline (Time-Travel & Schema Evolution)

This guide extends the Kafka-to-Parquet pipeline with Apache Iceberg, enabling: ‚úÖ Time-travel queries
‚úÖ Schema evolution (auto-update columns without breaking queries)
‚úÖ Optimized storage with metadata pruning
‚úÖ Support for Kafka semantics (at-least-once, exactly-once)
1Ô∏è‚É£ Choosing Kafka Processing Semantics (Exactly-Once vs. At-Least-Once)
üî• Kafka Consumer Guarantees

| **Guarantee** | **Behavior** | **Best Use Case** | **Trade-offs** |
|--------------|------------|----------------|-------------|
| **At-Least-Once** | Messages are processed **at least once** but may be duplicated | High-throughput pipelines where duplicates can be handled (e.g., **aggregations**) | May require **de-duplication** in Iceberg |
| **Exactly-Once** | Messages are processed **only once**, preventing duplicates | Critical workflows like **financial transactions** | Higher **latency & resource usage** |

üî• Configuring Kafka Read Semantics in Spark SQL

-- Enable at-least-once processing
SET spark.sql.streaming.kafka.useDeprecatedOffsetFetching = false;
SET spark.sql.streaming.commitOnBatchCompletion = true;

-- Enable exactly-once processing
SET spark.sql.streaming.kafka.useDeprecatedOffsetFetching = false;
SET spark.sql.streaming.commitOnBatchCompletion = false;

üí° Result: Selects the best trade-off between throughput and duplication prevention ‚úÖ
2Ô∏è‚É£ Ingest Kafka Data into Iceberg
üî• Best Practices for Iceberg Table Creation

| **Optimization** | **Solution** |
|-----------------|-------------|
| **Enable Iceberg `MERGE INTO` for deduplication** | Ensures idempotent writes in at-least-once processing. |
| **Use `copy-on-write` mode** | Best for frequent updates and low-latency queries. |
| **Optimize partitioning (`PARTITION BY` event date)** | Ensures fast filtering of recent events. |

üî• Example: Create Iceberg Table for Kafka Data

CREATE TABLE iceberg_transactions (
  user_id STRING,
  transaction_amount DOUBLE,
  event_time TIMESTAMP,
  event_date DATE
)
USING iceberg
PARTITIONED BY (event_date);

üí° Result: Table is optimized for fast reads and updates ‚úÖ
3Ô∏è‚É£ Process & Upsert Data in Iceberg
üî• Use MERGE INTO to Handle At-Least-Once Processing

```MERGE INTO iceberg_transactions target
USING (
  SELECT user_id, transaction_amount, event_time, CAST(event_time AS DATE) AS event_date
  FROM kafka_stream
) source
ON target.user_id = source.user_id AND target.event_time = source.event_time
WHEN MATCHED THEN
  UPDATE SET transaction_amount = source.transaction_amount
WHEN NOT MATCHED THEN
  INSERT (user_id, transaction_amount, event_time, event_date)
  VALUES (source.user_id, source.transaction_amount, source.event_time, source.event_date);
```
üí° Result: Avoids duplicates in at-least-once Kafka processing ‚úÖ
4Ô∏è‚É£ Time-Travel Queries with Iceberg
üî• Best Practices for Time Travel

| **Optimization**             | **Solution** |
|-----------------|-----------------

| **Use `AS OF TIMESTAMP` for historical queries** | Queries data **as it was at a specific time**. |
| **Use `VERSION AS OF` for rollback** | Restores a previous table state. |
| **Enable `SNAPSHOT_ID` pruning** | Reads only **relevant metadata**, improving query speed. |

üî• Example: Query Iceberg Data from 1 Hour Ago

SELECT * FROM iceberg_transactions TIMESTAMP AS OF now() - interval 1 hour;

üí° Result: Returns transactions as they were 1 hour ago ‚úÖ
üî• Example: Restore Iceberg Table to a Previous Version

ROLLBACK TO VERSION 123456789;

üí° Result: Fully restores previous state ‚úÖ
5Ô∏è‚É£ Optimize Iceberg Storage & Compaction
üî• Best Practices for Iceberg Performance

| **Optimization** | **Solution** |
|-----------------|-------------|
| **Enable metadata pruning** | Reads only **necessary partitions** for faster queries. |
| **Compact small files (`REWRITE DATAFILES`)** | Prevents the **small file problem**. |
| **Use `ZORDER` for better scan efficiency** | Clusters similar data together for better read performance. |

üî• Example: Compact Small Files in Iceberg

CALL iceberg_transactions.REWRITE_DATAFILES();

üí° Result: Improves query speed & reduces storage overhead ‚úÖ
üöÄ Final Summary: Kafka-to-Iceberg Pipeline Optimizations

| Step |                       Optimization |
|------|    -------------|
| **1Ô∏è‚É£ Kafka Read Semantics** | Choose **at-least-once (high throughput) or exactly-once (no duplicates)**. |
| **2Ô∏è‚É£ Iceberg Ingestion** | Use **`MERGE INTO`** to prevent duplicates in at-least-once processing. |
| **3Ô∏è‚É£ Time-Travel Queries** | Use **`TIMESTAMP AS OF`** to retrieve historical data. |
| **4Ô∏è‚É£ Storage Optimization** | Use **`REWRITE DATAFILES`** to compact small files. |



üöÄ Iceberg-Based Real-Time Aggregation in Spark SQL

This guide covers real-time streaming aggregations using Kafka + Iceberg in Spark SQL, including: ‚úÖ Kafka ingestion with Iceberg
‚úÖ Windowed aggregations (e.g., hourly, daily stats per user)
‚úÖ Incremental updates (MERGE INTO instead of full scans)
‚úÖ Optimized compaction & time-travel queries
1Ô∏è‚É£ Ingest Kafka Data into Iceberg
üî• Best Practices for Streaming Data Ingestion

| **Optimization** | **Solution** |
|-----------------|-------------|
| **Use `maxOffsetsPerTrigger` to control batch size** | Ensures Spark processes a fixed number of Kafka messages per batch. |
| **Enable checkpointing** | Prevents data loss & enables exactly-once processing. |
| **Use schema-on-read** | Supports evolving schemas without breaking queries. |

üî• Example: Read Kafka Events into Iceberg

CREATE TABLE kafka_stream 
USING kafka 
OPTIONS (
  bootstrap.servers = "kafka-broker:9092",
  subscribe = "transactions",
  startingOffsets = "latest",
  maxOffsetsPerTrigger = 10000
);

üí° Result: Only new messages are processed ‚úÖ
2Ô∏è‚É£ Aggregate Streaming Data in Real-Time
üî• Best Practices for Streaming Aggregations

| **Optimization** | **Solution** |
|-----------------|-------------|
| **Use `WINDOW` functions for time-based aggregations** | Reduces the need for full table scans. |
| **Use `MERGE INTO` for incremental updates** | Prevents redundant writes. |
| **Use `WATERMARK` for late-arriving events** | Ensures Spark discards old records gracefully. |

üî• Example: Compute Hourly Total Spending per User

CREATE TABLE user_hourly_spending 
USING iceberg 
PARTITIONED BY (event_date, hour) 
AS 
SELECT 
  user_id, 
  SUM(transaction_amount) AS total_spent, 
  DATE(event_time) AS event_date,
  HOUR(event_time) AS hour
FROM kafka_stream 
WHERE event_time >= WATERMARK(event_time, "10 minutes") 
GROUP BY user_id, event_date, hour;

üí° Result: Aggregates are computed in real-time and partitioned efficiently ‚úÖ
3Ô∏è‚É£ Merge Aggregated Data for Incremental Updates
üî• Best Practices for Merging Data Efficiently

| **Optimization** | **Solution** |
|-----------------|-------------|
| **Use `MERGE INTO` instead of full table overwrites** | Updates only changed records, reducing compute overhead. |
| **Use `coalesce()` before writing to avoid small files** | Prevents fragmentation in storage. |
| **Enable `ZORDER` for better scan efficiency** | Groups frequently accessed data together for fast reads. |

üî• Example: Merge Aggregated Data Without Duplication

MERGE INTO user_hourly_spending target
USING (
  SELECT user_id, SUM(transaction_amount) AS total_spent, DATE(event_time) AS event_date, HOUR(event_time) AS hour
  FROM kafka_stream
  GROUP BY user_id, event_date, hour
) source
ON target.user_id = source.user_id 
AND target.event_date = source.event_date 
AND target.hour = source.hour
WHEN MATCHED THEN 
  UPDATE SET total_spent = source.total_spent
WHEN NOT MATCHED THEN 
  INSERT (user_id, total_spent, event_date, hour)
  VALUES (source.user_id, source.total_spent, source.event_date, source.hour);

üí° Result: Updates only modified records, reducing write amplification ‚úÖ
4Ô∏è‚É£ Query Aggregated Data with Time Travel
üî• Best Practices for Time-Travel Queries

| **Optimization** | **Solution** |
|-----------------|-------------|
| **Use `VERSION AS OF` to rollback tables** | Allows you to restore previous states easily. |
| **Use `SNAPSHOT_ID` to track incremental changes** | Enables fine-grained control over table revisions. |
| **Use `ZORDER` indexing** | Improves query speed by clustering similar data together. |

üî• Example: Query Aggregates from 1 Day Ago

``` SELECT * FROM user_hourly_spending TIMESTAMP AS OF now() - interval 1 day; ```

üí° Result: Returns hourly spending statistics as they were yesterday ‚úÖ
üî• Example: Restore Table to a Previous Snapshot

ROLLBACK TO VERSION 123456789;

üí° Result: Fully restores previous state ‚úÖ
5Ô∏è‚É£ Optimize Iceberg Storage & Compaction
üî• Best Practices for Reducing Storage Overhead

| **Optimization** | **Solution** |
|-----------------|-------------|
| **Enable metadata pruning** | Reads only **necessary partitions** for faster queries. |
| **Compact small files (`REWRITE DATAFILES`)** | Reduces the number of storage files. |
| **Use `ZORDER` for better scan efficiency** | Groups similar data together for better read performance. |

üî• Example: Compact Small Files in Iceberg

CALL user_hourly_spending.REWRITE_DATAFILES();

üí° Result: Improves query speed & reduces storage overhead ‚úÖ
üöÄ Final Summary: Iceberg-Based Streaming Aggregation

| Step | Optimization |
|------|-------------|
| **1Ô∏è‚É£ Kafka Ingestion** | Use `maxOffsetsPerTrigger` and checkpointing for **efficient streaming reads**. |
| **2Ô∏è‚É£ Real-Time Aggregations** | Use `WINDOW` functions and `WATERMARK` for **time-based stats**. |
| **3Ô∏è‚É£ Merging Aggregates** | Use `MERGE INTO` to **avoid full table overwrites**. |
| **4Ô∏è‚É£ Time-Travel Queries** | Use `TIMESTAMP AS OF` to **query historical data**. |
| **5Ô∏è‚É£ Storage Optimization** | Use **`REWRITE DATAFILES`** to **compact small files**. |


1Ô∏è‚É£ How This Solution Handles At-Least-Once Processing
üî• Key Strategies Used

| **Issue** | **Solution** |
|----------|-------------|
| **Duplicate records due to Kafka's at-least-once semantics** | Use `MERGE INTO` to **update or insert only new records**. |
| **Re-processing on failure** | Use **checkpointing** to store processed offsets. |
| **Data integrity issues** | Use **event-time watermarking** to prevent old duplicate events from being processed. |

2Ô∏è‚É£ Ensuring Exactly-Once Updates in Iceberg
üî• Why Use MERGE INTO Instead of INSERT INTO?

    INSERT INTO blindly appends duplicate records.
    MERGE INTO updates existing records if they already exist, preventing duplicate writes.

üî• Example: De-duplicating Streaming Data in Iceberg
```
MERGE INTO user_hourly_spending target
USING (
  SELECT user_id, SUM(transaction_amount) AS total_spent, DATE(event_time) AS event_date, HOUR(event_time) AS hour
  FROM kafka_stream
  GROUP BY user_id, event_date, hour
) source
ON target.user_id = source.user_id 
AND target.event_date = source.event_date 
AND target.hour = source.hour
WHEN MATCHED THEN 
  UPDATE SET total_spent = source.total_spent
WHEN NOT MATCHED THEN 
  INSERT (user_id, total_spent, event_date, hour)
  VALUES (source.user_id, source.total_spent, source.event_date, source.hour);
```

‚úÖ Ensures that each user‚Äôs hourly aggregation is updated instead of duplicated.
3Ô∏è‚É£ Enabling Checkpointing to Avoid Reprocessing
üî• Why Use Checkpointing?

    Without checkpointing, if Spark Streaming fails and restarts, it will reprocess Kafka messages.
    Checkpointing stores offsets so that Spark resumes from where it left off.

üî• Example: Checkpointing Streaming Writes
```
CREATE TABLE iceberg_transactions 
USING iceberg 
OPTIONS (
  path = "s3://my-bucket/transactions/",
  checkpointLocation = "s3://my-bucket/spark-checkpoints/"
) 
AS 
SELECT * FROM kafka_stream;
```
‚úÖ Guarantees Spark resumes exactly where it left off after failures.
4Ô∏è‚É£ Handling Late-Arriving Data with Watermarking
üî• Why Use WATERMARK?

    Prevents reprocessing very old events that Kafka may redeliver.
    Ensures only new, relevant events are processed.

üî• Example: Filtering Late Events

``` SELECT * FROM kafka_stream 
WHERE event_time >= WATERMARK(event_time, "10 minutes");
```
‚úÖ Ensures old duplicates don‚Äôt pollute the dataset.

üöÄ Final Answer: Does This Solve Kafka‚Äôs At-Least-Once Semantics?

‚úÖ Yes, the combination of MERGE INTO, checkpointing, and watermarking ensures Spark processes Kafka‚Äôs at-least-once delivery without duplicates.
üöÄ Next Steps

Would you like an Iceberg streaming ingestion DAG using Airflow for better orchestration? üöÄ
You said:
does SELECT * FROM kafka_stream is from a table named kafka_stream or from the stream itself ?
ChatGPT said:
ChatGPT
Is SELECT * FROM kafka_stream Reading from a Table or a Stream?

‚úÖ It depends on how kafka_stream is defined.
1Ô∏è‚É£ If kafka_stream is a Temporary Table (View) on a Kafka Stream

If kafka_stream was created using a direct Kafka source (e.g., USING kafka), then it represents a real-time stream, not a static table.

Example: Creating a Streaming Kafka Table

CREATE TABLE kafka_stream 
USING kafka 
OPTIONS (
  bootstrap.servers = "kafka-broker:9092",
  subscribe = "transactions",
  startingOffsets = "latest",
  maxOffsetsPerTrigger = 10000
);

üí° In this case, SELECT * FROM kafka_stream reads from the live Kafka stream.
2Ô∏è‚É£ If kafka_stream is a Materialized Iceberg Table

If kafka_stream was previously ingested into an Iceberg table, then it is a batch table, not a real-time stream.

Example: Persisting Kafka Data into Iceberg

CREATE TABLE iceberg_transactions 
USING iceberg 
AS SELECT * FROM kafka_stream;

üí° In this case, SELECT * FROM kafka_stream reads from a static Iceberg table, not a stream.
üî• How to Check if kafka_stream is a Stream or a Table

Run:

DESCRIBE TABLE kafka_stream;

    If it lists Kafka options (e.g., bootstrap.servers, subscribe), it‚Äôs a stream.
    If it lists schema details without Kafka configs, it‚Äôs a table.

üöÄ Summary: How SELECT * FROM kafka_stream Works

| Scenario | `kafka_stream` Represents | Behavior of `SELECT * FROM kafka_stream` |
|----------|--------------------------|-------------------------------------------|
| **Created using `USING kafka`** | A real-time **stream** from Kafka | Fetches **new Kafka messages** continuously |
| **Created as a persisted table (`USING iceberg`)** | A **static** Iceberg table | Reads **historical stored data** |

Would you like an example of how to ingest streaming data into an Iceberg table while keeping a live stream running? üöÄ
You said:
yes
ChatGPT said:
ChatGPT
üöÄ Streaming Data Ingestion into Iceberg While Keeping a Live Stream Running

This guide demonstrates how to:
‚úÖ Continuously ingest Kafka events into an Iceberg table
‚úÖ Query live streaming data & stored Iceberg data simultaneously
‚úÖ Ensure exactly-once processing with checkpointing
1Ô∏è‚É£ Set Up a Continuous Kafka Stream in Spark SQL
üî• Create a Real-Time Kafka Stream Table

CREATE TABLE kafka_stream 
USING kafka 
OPTIONS (
  bootstrap.servers = "kafka-broker:9092",
  subscribe = "transactions",
  startingOffsets = "latest",
  maxOffsetsPerTrigger = 10000
);

‚úÖ Now, kafka_stream represents a real-time data stream from Kafka.
2Ô∏è‚É£ Continuously Ingest Streaming Data into Iceberg
üî• Persist Kafka Data into an Iceberg Table

CREATE TABLE iceberg_transactions 
USING iceberg 
PARTITIONED BY (event_date)
OPTIONS (
  path = "s3://my-bucket/transactions/",
  checkpointLocation = "s3://my-bucket/spark-checkpoints/"
) 
AS 
SELECT 
  user_id, 
  transaction_amount, 
  event_time, 
  DATE(event_time) AS event_date
FROM kafka_stream;

‚úÖ Now, all new Kafka events are continuously stored in Iceberg.
3Ô∏è‚É£ Query Both Live Stream & Stored Data
üî• Querying Live Kafka Events (Stream)

SELECT * FROM kafka_stream 
WHERE event_time >= WATERMARK(event_time, "10 minutes");

‚úÖ Returns real-time events from Kafka.
üî• Querying Stored Iceberg Data

SELECT * FROM iceberg_transactions 
WHERE event_date = CURRENT_DATE;

‚úÖ Returns historical Iceberg data stored in S3.
4Ô∏è‚É£ Querying Both Live & Stored Data Together
üî• Unifying Real-Time & Stored Data

SELECT user_id, transaction_amount, event_time 
FROM kafka_stream
UNION ALL
SELECT user_id, transaction_amount, event_time 
FROM iceberg_transactions 
WHERE event_date = CURRENT_DATE;

‚úÖ Combines live Kafka events & stored Iceberg data for a unified view.
5Ô∏è‚É£ Handle Late Events & Deduplication
üî• Preventing Late Duplicates

MERGE INTO iceberg_transactions target
USING (
  SELECT user_id, SUM(transaction_amount) AS total_spent, event_date
  FROM kafka_stream
  GROUP BY user_id, event_date
) source
ON target.user_id = source.user_id AND target.event_date = source.event_date
WHEN MATCHED THEN 
  UPDATE SET transaction_amount = source.total_spent
WHEN NOT MATCHED THEN 
  INSERT (user_id, transaction_amount, event_date)
  VALUES (source.user_id, source.total_spent, source.event_date);

‚úÖ Prevents duplicate writes while ensuring all data is up to date.
üöÄ Final Summary: Iceberg Streaming Ingestion

| Step | Optimization |
|------|-------------|
| **1Ô∏è‚É£ Create Kafka Stream** | Use `USING kafka` to define a **live streaming table**. |
| **2Ô∏è‚É£ Store Data in Iceberg** | Use `USING iceberg` to create a **persistent table**. |
| **3Ô∏è‚É£ Query Both Live & Stored Data** | Use `UNION ALL` to **combine streaming + historical data**. |
| **4Ô∏è‚É£ Prevent Duplicates** | Use `MERGE INTO` to **deduplicate Kafka reprocessing**. |








